{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FogE16raHaDp"
   },
   "source": [
    "# 20 News Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "nRK_Ip5kPbBO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "categories = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "\n",
    "#Try basic load for the form of input necessary for bayes\n",
    "#x_train, y_train = newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), return_X_y=True, random_state=42)\n",
    "#x_test, y_test = newsgroups_train = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), return_X_y=True, random_state=42)\n",
    "\n",
    "x, y  = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), return_X_y=True, random_state=42)\n",
    "\n",
    "#VECTORIZATION\n",
    "#COUNT\n",
    "vectorizer_count = CountVectorizer(stop_words='english', strip_accents='ascii')\n",
    "count_newsgroup_x = vectorizer_count.fit_transform(x)\n",
    "words_as_features_count = vectorizer_count.get_feature_names_out()\n",
    "print(words_as_features_count[50:100])\n",
    "print(count_newsgroup_x)\n",
    "print(count_newsgroup_x.shape)\n",
    "\n",
    "#TFID\n",
    "#vectorize to make each word a feature\n",
    "vectorizer_tfid = TfidfVectorizer(stop_words='english', strip_accents='ascii')\n",
    "tfid_newsgroup_x = vectorizer_tfid.fit_transform(x)\n",
    "words_as_features_tfid = vectorizer_tfid.get_feature_names_out()\n",
    "print(words_as_features_tfid[50:100])\n",
    "print(tfid_newsgroup_x)\n",
    "print(tfid_newsgroup_x.shape)\n",
    "\n",
    "#TEST TRAIN SPLIT\n",
    "TRAIN_PERCENTAGE = 80 \n",
    "#Count Data\n",
    "num_instances_count = count_newsgroup_x.shape[0]\n",
    "splitIndex = math.floor(num_instances_count*(TRAIN_PERCENTAGE/100))\n",
    "count_newsgroup_x_train, count_newsgroup_x_test = count_newsgroup_x[:splitIndex], count_newsgroup_x[splitIndex:]\n",
    "count_newsgroup_y_train, count_newsgroup_y_test = y[:splitIndex], y[splitIndex:]\n",
    "\n",
    "#Tfid Data\n",
    "num_instances_tfid = tfid_newsgroup_x.shape[0]\n",
    "splitIndex = math.floor(num_instances_tfid*(TRAIN_PERCENTAGE/100))\n",
    "tfid_newsgroup_x_train, tfid_newsgroup_x_test = tfid_newsgroup_x[:splitIndex], tfid_newsgroup_x[splitIndex:]\n",
    "tfid_newsgroup_y_train, tfid_newsgroup_y_test = y[:splitIndex], y[splitIndex:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrMILtmNslIu"
   },
   "source": [
    "## 20 News Group Notes\n",
    "\n",
    "### Accessible variables from this cell:\n",
    "\n",
    "2 Versions of our data:\n",
    "\n",
    "> COUNT VECTORIZED:\n",
    "\n",
    "*    X TRAIN:   count_newsgroup_x_train\n",
    "*    Y TRAIN:   count_newsgroup_y_train\n",
    "*    X TEST:    count_newsgroup_x_test\n",
    "*    Y TEST:    count_newsgroup_y_test\n",
    "\n",
    "> TFID\n",
    "\n",
    "\n",
    "*    X TRAIN:   tfid_newsgroup_x_train\n",
    "*    Y TRAIN:   tfid_newsgroup_y_train\n",
    "*    X TEST:    tfid_newsgroup_x_test\n",
    "*    Y TEST:    tfid_newsgroup_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbQ7n71zugq1"
   },
   "source": [
    "# Sentiment 140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n65ucM-teBwC"
   },
   "source": [
    "## Variables Available from this Cell\n",
    "\n",
    ">COUNT VECTORIZED\n",
    "\n",
    "* X TRAIN: count_sentiment_x_train\n",
    "* Y TRAIN: count_sentiment_y_train\n",
    "* X TEST: count_sentiment_x_test\n",
    "* Y TEST: count_sentiment_y_test\n",
    "\n",
    ">TFID\n",
    "\n",
    "* X TRAIN:   tfid_sentiment_x_train\n",
    "* Y TRAIN:   tfid_sentiment_y_train\n",
    "* X TEST:    tfid_sentiment_x_test\n",
    "* Y TEST:    tfid_sentiment_y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuKwN0yAeLd4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google.colab import files\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "\n",
    "#uploaded = files.upload()\n",
    "\n",
    "#COUNT VECTORIZED\n",
    "count_sentiment_x_train = scipy.sparse.load_npz(file = '/content/count_sentiment_x_train.npz')\n",
    "count_sentiment_y_train = np.load(file = '/content/count_sentiment_y_train.npy')\n",
    "count_sentiment_x_test = scipy.sparse.load_npz(file = '/content/count_sentiment_x_test.npz')\n",
    "count_sentiment_y_test = np.load(file = '/content/count_sentiment_y_test.npy')\n",
    "\n",
    "count_sentiment = (count_sentiment_x_train,  count_sentiment_y_train, count_sentiment_x_test, count_sentiment_y_test)\n",
    "\n",
    "#TFID\n",
    "tfid_sentiment_x_train = scipy.sparse.load_npz(file = '/content/tfid_sentiment_x_train.npz')\n",
    "tfid_sentiment_y_train = np.load(file = '/content/tfid_sentiment_y_train.npy')\n",
    "tfid_sentiment_x_test = scipy.sparse.load_npz(file = '/content/tfid_sentiment_x_test.npz')\n",
    "tfid_sentiment_y_test = np.load(file = '/content/tfid_sentiment_y_test.npy')\n",
    "\n",
    "tfid_sentiment = (tfid_sentiment_x_train, tfid_sentiment_y_train, tfid_sentiment_x_test, tfid_sentiment_y_test)\n",
    "\n",
    "print(count_sentiment_y_train[:10])\n",
    "print(count_sentiment_x_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cu5VGVt5za7K"
   },
   "source": [
    "## Sentiment 140 Notes\n",
    "\n",
    "sentiment_train.csv == the FULL train data from sentiment 140 website\n",
    "\n",
    "sentiment_test.csv == the SUBSET of test data provided in Comp551>Announcements>P2 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCHeyvW4uXiX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import math\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from io import StringIO\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "import requests\n",
    "from io import StringIO\n",
    "import random\n",
    "import sys\n",
    "import csv\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "#uncomment for prompt to uploadfile\n",
    "uploaded = files.upload()\n",
    "\n",
    "#load the csv into a pandas dataframe, attributes are not labelled columns (target is the class or label), shuffle\n",
    "#train\n",
    "sentiment_onefourty_train = pd.read_csv('sentiment_train.csv', header= 0, delimiter=\",\", nrows=100000,encoding_errors='ignore', usecols=[0,5],skiprows=([i for i in range(0, 750000)] + [j for j in range(950000, 1600000)]))   \n",
    "sentiment_onefourty_train.columns=['target', 'text']\n",
    "\n",
    "#test\n",
    "sentiment_onefourty_test = pd.read_csv(filepath_or_buffer=r'sentiment_test.csv', header= 0, delimiter=\",\", encoding_errors='ignore', usecols=[0,5])\n",
    "sentiment_onefourty_test.columns=['target', 'text']\n",
    "\n",
    "#shuffle\n",
    "sentiment_onefourty_train = sentiment_onefourty_train.sample(frac=1, random_state = 1234)\n",
    "sentiment_onefourty_test = sentiment_onefourty_test.sample(frac=1, random_state = 1234)\n",
    "\n",
    "#extract the target (class values) to numpy array and convert to 0's and 1's (instead of 0,4)\n",
    "#train\n",
    "sentiment_onefourty_y_train = sentiment_onefourty_train['target'].to_numpy(copy=True)\n",
    "sentiment_onefourty_y_train = np.where(sentiment_onefourty_y_train == 4, 1, sentiment_onefourty_y_train)\n",
    "#test\n",
    "sentiment_onefourty_y_test = sentiment_onefourty_test['target'].to_numpy(copy=True)\n",
    "sentiment_onefourty_y_test = np.where(sentiment_onefourty_y_test ==4, 1, sentiment_onefourty_y_test)\n",
    "\n",
    "#same things but keep naming convetion to avoid confusion\n",
    "tfid_sentiment_y_train = sentiment_onefourty_y_train\n",
    "count_sentiment_y_train = sentiment_onefourty_y_train\n",
    "tfid_sentiment_y_test = sentiment_onefourty_y_test\n",
    "count_sentiment_y_test = sentiment_onefourty_y_test\n",
    "\n",
    "#CREATE COUNT AND TFID VECTORIZED DATA SETS\n",
    "\n",
    "#extract the text arrays from the pandas dataframe 'text' column (for both test and train)\n",
    "sentiment_onefourty_text_train= sentiment_onefourty_train['text'].to_numpy(copy=True)\n",
    "sentiment_onefourty_text_test= sentiment_onefourty_test['text'].to_numpy(copy=True)\n",
    "\n",
    "\n",
    "#Count Vectorizer\n",
    "#train set\n",
    "vectorizer_count_train = CountVectorizer(stop_words='english', strip_accents='ascii') #min_df= 0.013 ? after PCA\n",
    "count_sentiment_x_train = vectorizer_count_train.fit_transform(sentiment_onefourty_text_train)\n",
    "words_as_features_count_train = vectorizer_count_train.get_feature_names_out()\n",
    "\n",
    "#test set\n",
    "vectorizer_count_test = CountVectorizer(vocabulary =words_as_features_count_train)\n",
    "count_sentiment_x_test = vectorizer_count_test.fit_transform(sentiment_onefourty_text_test)\n",
    "words_as_features_count_test = vectorizer_count_test.get_feature_names_out()\n",
    "\n",
    "#Tfid vectorizer\n",
    "vectorizer_tfid_train = TfidfVectorizer(stop_words='english', strip_accents='ascii')\n",
    "tfid_sentiment_x_train = vectorizer_tfid_train.fit_transform(sentiment_onefourty_text_train)\n",
    "words_as_features_tfid_train = vectorizer_tfid_train.get_feature_names_out()\n",
    "\n",
    "vectorizer_tfid_test = TfidfVectorizer(vocabulary = words_as_features_tfid_train)\n",
    "tfid_sentiment_x_test = vectorizer_tfid_test.fit_transform(sentiment_onefourty_text_test)\n",
    "words_as_features_tfid_test = vectorizer_tfid_test.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(\"train\")\n",
    "print(sentiment_onefourty_train['target'].value_counts())\n",
    "print(\"test\")\n",
    "print(sentiment_onefourty_test['target'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "T1-DataLoading,Cleaning.ipynb",
   "provenance": [
    {
     "file_id": "1MMTk2t_xsSIxUxIc1C_aVMGARSLjRFAJ",
     "timestamp": 1646692502448
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
